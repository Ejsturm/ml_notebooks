{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook was set up by Erica Sturm. Send your questions and comments to esturm@bnl.gov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "complete_path = os.getcwd()\n",
    "if 'nb' in complete_path:\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from IPython.display import clear_output\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import time\n",
    "from mlnrg.utils.logger import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "from mlnrg.utils.mpl_at_utils import MPLAdjutant\n",
    "from mlnrg.loader import NRGData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Anderson versus Kondo\n",
    "\n",
    "#kondo = pd.read_csv('data/Kondo_411267_trials_with_headers.csv')\n",
    "anderson = pd.read_csv('data/Anderson_599578_trials_with_headers.csv')\n",
    "#data_set = pd.read_csv('data/Kondo_411267_trials_with_headers.csv')\n",
    "#data_set = pd.read_csv('/Users/erica/Documents/Research/2channel_anderson/First_17k_2CHAM_trials_with_headers.csv')\n",
    "\n",
    "# For plotting\n",
    "#grid = 'pd.read_csv(data/Omega_grid_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose either furthest point sampling (FPS) or random point sampling (RPS) to train and test with. It is important to use these (without shfffling) so that the trials in the training/validation/test sets are the same as those in the NN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fps_indexes = pickle.load(open(\"results/fps_files/kondo_fps.pkl\", \"rb\"))\n",
    "#rps_indexes = pickle.load(open(\"results/fps_files/anderson_random.pkl\", \"rb\"))\n",
    "point_indexes = pickle.load(open(\"results/fps_files/anderson_fps.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify number of trials to be used in training. Default is 50K!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_used_trials = 49408 # Default = 50000\n",
    "#all_training_idx = list(point_indexes[0]['train'][:total_used_trials])\n",
    "all_training_idx = np.array(point_indexes[0]['train'][:])\n",
    "all_validation_idx = list(point_indexes[0]['valid'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt(\"All_Anderson_training_data.csv\", all_training_idx[:], delimiter=\",\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = (anderson.loc[all_training_idx[:], :]).to_numpy()\n",
    "savetxt(\"All_Anderson_training_data.csv\",foo[:], delimiter=\",\", fmt='%1.8f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symlog scaling here; comment or uncomment as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start with B: rescale with symlog after adjusting for B=0 cases.\n",
    "# m = (data_set[data_set['B'] != 0]['B']).abs().min()\n",
    "# data_set.loc[list((data_set[data_set['B'] == 0]).index), 'B'] = m/10\n",
    "\n",
    "# sign_B = np.sign(data_set['B'])\n",
    "# data_set.loc[:, 'B'] = sign_B * np.log10(np.abs(data_set.loc[:, 'B']))\n",
    "# #data_set.rename(columns={'B': 'symlog10 B'}, inplace=True)\n",
    "\n",
    "\n",
    "# # Now do T: rescale with regular log after adjusting for T=0 cases.\n",
    "# m = (data_set[data_set['T'] != 0]['T']).abs().min()\n",
    "# data_set.loc[list((data_set[data_set['T'] == 0]).index), 'T'] = m/10\n",
    "# data_set.loc[:, 'T'] = np.log10(data_set.loc[:, 'T'])\n",
    "# #data_set.rename(columns={'T': 'log10 T'}, inplace=True)\n",
    "\n",
    "# # Now do Gamma: recale with regular log.\n",
    "# data_set.loc[:, 'Gamma'] = np.log10(data_set.loc[:, 'Gamma'])\n",
    "# #data_set.rename(columns={'Gamma': 'log10 Gamma'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscaled_tr_features = (data_set.loc[all_training_idx,'U':'T']).to_numpy()\n",
    "# tr_targets = (data_set.iloc[all_training_idx, 8:]).to_numpy()\n",
    "\n",
    "# unscaled_val_features = (data_set.loc[all_validation_idx,'U':'T']).to_numpy()\n",
    "# val_targets = (data_set.iloc[all_validation_idx, 8:]).to_numpy()\n",
    "\n",
    "unscaled_tr_features = (data_set.iloc[:16491,1:9]).to_numpy()\n",
    "tr_targets = (data_set.iloc[:16491, 15:]).to_numpy()\n",
    "\n",
    "unscaled_val_features = (data_set.iloc[16491:,1:9]).to_numpy()\n",
    "val_targets = (data_set.iloc[16491:, 15:]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_scaling = unscaled_tr_features.mean(axis=0)\n",
    "std_scaling = unscaled_tr_features.std(axis=0)\n",
    "\n",
    "tr_features = (unscaled_tr_features - mu_scaling) / std_scaling\n",
    "val_features = (unscaled_val_features - mu_scaling) / std_scaling\n",
    "\n",
    "\n",
    "unscaled_val_mean = np.mean(unscaled_val_features.mean(axis=0))\n",
    "unscaled_val_std = np.mean(unscaled_val_features.std(axis=0))\n",
    "\n",
    "rescaled_tr_mean = np.mean(tr_features.mean(axis=0))\n",
    "rescaled_tr_std = np.mean(tr_features.std(axis=0))\n",
    "rescaled_val_mean = np.mean(val_features.mean(axis=0))\n",
    "rescaled_val_std = np.mean(val_features.std(axis=0))\n",
    "\n",
    "log.info(\"\\nThe original mean +/- std of the training features: %.4f +/- %.4f.\\n\" \\\n",
    "         \"The rescaled mean +/- std of the training features: %.4f +/- %.4f.\\n\\n\" \\\n",
    "         \"The original mean +/- std of the validation features: %.4f +/- %.4f.\\n\" \\\n",
    "         \"The rescaled mean +/- std of the validation features: %.4f +/- %.4f.\"\n",
    "         % (np.mean(mu_scaling), np.mean(std_scaling), \n",
    "           rescaled_tr_mean, rescaled_tr_std,\n",
    "           unscaled_val_mean, unscaled_val_std,\n",
    "           rescaled_val_mean, rescaled_val_std\n",
    "           )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unscaled_tr_features.mean(axis=0))\n",
    "print(unscaled_tr_features.std(axis=0))\n",
    "print(unscaled_val_features.mean(axis=0))\n",
    "print(unscaled_val_features.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_features.mean(axis=0))\n",
    "print(val_features.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model, train, and run validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the KRR model: choose values for \"gamma\" the kernel radius and \"alpha\" the regularization term. The kernel is a constant set to \"Laplacian\": $k(x,y) = \\mathrm{exp}(-\\gamma||x-y||_1)$. The defaults are gamma=0.1 and alpha=0 (the same as Arsenault's values). Note that in the literature the kernel radius is denoted $\\frac{1}{\\sigma}$ and the regularization term is $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01  # Default = 0.0; other choices: [0.0, 0.01, 0.1]\n",
    "gamma = 0.1 # Default = 0.1; other choices: [0.01, 0.1, 1]\n",
    "\n",
    "# Set up KRR model\n",
    "krrModel = KernelRidge(kernel='laplacian', gamma=gamma, alpha=alpha)\n",
    "t0 = time.time()\n",
    "# Fit the training data.\n",
    "trained_model = krrModel.fit(tr_features, tr_targets)\n",
    "fit_time = time.time() - t0\n",
    "log.info(\"\\nKRR model with Laplacian radius %.5g and regularization %.5g took %3.2g seconds to train on %d trials.\" \n",
    "      % (gamma, alpha, fit_time, len(tr_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions abou the validation set using the trained KRR model and compute errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "val_predictions = trained_model.predict(val_features[:,:])\n",
    "prediction_time = time.time() - t0\n",
    "log.info(\"\\nThe prediction time for %d validation trials took %.2g seconds.\" %(len(val_features), prediction_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    tr_cost = trained_model.score(tr_features, tr_targets)\n",
    "    val_cost = trained_model.score(val_features, val_targets)\n",
    "    log.info(\"\\nThe cost of the training data (should be 1.0):\\t%.5g\\nThe cost of the validation data:\\t\\t%.5g\"\n",
    "     %(tr_cost, val_cost)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.mean(np.abs(val_targets - val_predictions), axis=1)\n",
    "sorted_errors = []\n",
    "for ii in range(len(errors)):\n",
    "  sorted_errors.append([ii, val_targets[ii], val_predictions[ii], errors[ii]])\n",
    "\n",
    "sorted_errors.sort(key=lambda x: x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "y_Height = 0.05 * len(errors)\n",
    "mean = np.log10(np.mean(errors))\n",
    "median = np.log10(np.median(errors))\n",
    "\n",
    "plt.hist(np.log10(errors), bins=60, color='k')\n",
    "for ii in range(1, 10):\n",
    "    x = np.log10(np.percentile(errors, ii*10))\n",
    "    plt.plot((x, x), (0, y_Height), color='r', label=\"Deciles\" if ii == 1 else None)\n",
    "plt.plot((mean, mean), (0, y_Height), color='orange', label=f\"Mean: {mean:.05g}\")\n",
    "plt.plot((median, median), (0, y_Height), color='b', label=f\"Median: {median:.05g}\")\n",
    "\n",
    "plt.title(\"%d Anderson Trials KRR MAE\" % len(errors))\n",
    "plt.xlabel(\"$log_{10}(\\mathrm{MAE})$\")\n",
    "plt.ylabel(\"Occurances\")\n",
    "plt.legend(loc=\"upper left\", frameon=False)\n",
    "\n",
    "#plt.savefig(\"Hist_anderson_fps_a\" + str(alpha) + \"_g\" + str(gamma) +\"_errors.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr_info = {\n",
    "    \"set\": \"kondo\",\n",
    "    \"sampling\": \"fps\",\n",
    "    \"tr_set_size\": len(tr_features),\n",
    "    \"val_set_size\": len(val_features),\n",
    "    \"trained_model\": trained_model,\n",
    "    \"fit_time\": fit_time,\n",
    "    \"sorted_errors\": sorted_errors,\n",
    "    \"tr_cost\": tr_cost,\n",
    "    \"val_cost\": val_cost\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODIFY THIS CELL TO ENSURE YOU DO NOT OVERWRITE A PREVIOUS SAVE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(krr_info, open(\"results/DC-KRR/KRR_kondo_fps_alpha0_gamma1_subset1_comparison.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(errors)\n",
    "median = np.median(errors)\n",
    "std = np.std(errors)\n",
    "log.info(\"\\nThe validation set's median: %.5f\\nand mean +/- std: %.5f +/- %.5f\" \\\n",
    "         \"\\n\\nThe same, but log10: median: %.5f\\nmean +/- std: %.5f +/- %.5f\"\n",
    "        % (median, mean, std, np.log10(median), np.log10(mean), np.log10(std))\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
